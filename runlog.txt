# RUN pure-meadow

python train.py --model qwen3-0.6b --gen-dataset eac123/openhermes-dpo-qwen3-30ba3b-120ksamples-prepared-generation --det-dataset eac123/openhermes-dpo-qwen3-30ba3b-120ksamples-prepared-detection --max-train-samples 512   --batch-size 1   --grad-accum 8   --optim-8bit   --max-seq-length 512

A100
0.6B pure meadow

36% usage
8.57GB vram

3:20 time


# qwen31.7b-dark-river-0129

BATCH_SIZE=4 MAX_SEQ_LENGTH=1024 STEG_EVAL_BATCH_SIZE=4 USE_8BIT_OPTIM=false MAX_TRAIN
_SAMPLES=512   ./scripts/train_1.7b_a100.sh


512 samples took 1:25?


# qwen31.7b-young-cliff-0129

scaling up number of samples by 40x

STEG_EVAL_STEPS=5000 BATCH_SIZE=4 MAX_SEQ_LENGTH=1024 STEG_EVAL_BATCH_SIZE=4 USE_8BIT_
OPTIM=false MAX_TRAIN_SAMPLES=20000   ./scripts/train_1.7b_a100.sh


5000 eval steps was too slow, this setup does 625 steps
so maybe 100 is good.

13 mins wasted, restarting with eval at 100 steps.


# qwen31.7b-light-prairie-0129

this run went to completion but failed to learn anything.
that's not good news. Especially as the run took a long time.

I'm going to try filtering the data more heavily and running that again.


python prepare_datasets.py --source eac123/openhermes-dpo-qwen3-30ba3b-120ksamples \
    --output-prefix eac123/steg-orpo-strict --min-alignment 0.70

GEN_DATASET=eac123/steg-orpo-strict-generation DET_DATASET=eac123/steg-orpo-strict-detection

this training run is half the size because of our strict filtering.









# qwen31.7b-light-island-0129

GEN_DATASET=eac123/steg-orpo-mixed-generation DET_DATASET=eac123/steg-orpo-mixed-detection











# 30th

A10
python train.py   --trainer sft   --det-dataset eac123/steg-orpo-strict-detection   --lr 5e-6   --epochs 1   --max-train-samples 10000   --steg-eval-steps 50 --wandb-project steg-sft 

